{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd \n",
    "import itertools\n",
    "from collections import Counter\n",
    "from pyvi.pyvi import ViTokenizer, ViPosTagger\n",
    "import utils\n",
    "import unicodecsv as csv\n",
    "import sys\n",
    "import json\n",
    "\n",
    "SOURCE_DIR = './../../crawl_info/user_post/'\n",
    "\n",
    "def tokenize(posts_str):\n",
    "    posts = posts_str.split('(^-^)')\n",
    "    result = []\n",
    "    for p in posts:\n",
    "        clean_str = utils.process(p) #.encode('utf-8')\n",
    "        result.extend(ViTokenizer.tokenize(clean_str).split(' '))\n",
    "    return result\n",
    "\n",
    "def data_load():\n",
    "    train_df = pd.read_csv(SOURCE_DIR + 'post_user_from_04_2017.csv')\n",
    "    train_df = train_df.dropna()\n",
    "    train_df = utils.add_age_category_to_df(train_df)\n",
    "    train_df['post'] = map(lambda x: x.decode('utf-8'), train_df['post'])\n",
    "    train_df.to_csv('./sample.csv', index=None, encoding = 'utf-8')\n",
    "    print \"Done data loading\"\n",
    "\n",
    "def text_and_labels():\n",
    "    df = pd.read_pickle('./tokenized_sample.pkl')\n",
    "    df = df.dropna()\n",
    "    X = df.loc[:, 'userposts']\n",
    "    y = df.loc[:, 'age']\n",
    "    print \"Done data loading\"\n",
    "    return X, y\n",
    "\n",
    "def save_data_and_labels(t_sentences, labels):\n",
    "    \"\"\"\n",
    "    Loads and preprocessed data for the MR dataset.\n",
    "    Returns input vectors, labels, vocabulary, and inverse vocabulary.\n",
    "    \"\"\"\n",
    "    # Load and preprocess data\n",
    "    #data_load()\n",
    "    df = pd.DataFrame(\n",
    "    {'userposts': t_sentences,\n",
    "     'age': labels\n",
    "    })\n",
    "    #pickle.dump(df, open('./CarDataset/original_dataset.pkl', 'wb'))\n",
    "    df.to_pickle('tokenized_sample.pkl')\n",
    "\n",
    "def load_data_and_labels():\n",
    "    \"\"\"\n",
    "    Loads MR polarity data from files, splits the data into words and generates labels.\n",
    "    Returns split sentences and labels.\n",
    "    \"\"\"\n",
    "    # Load data from files\n",
    "    data_load()\n",
    "    df = pd.read_csv('./sample.csv')\n",
    "    x_text = df.loc[:, 'post']\n",
    "    y_cat = df.loc[:, 'age_category']\n",
    "    x_text = map(lambda x: x.decode('utf-8'), x_text)\n",
    "    \n",
    "    # Split by words\n",
    "    X = [tokenize(user_posts) for user_posts in x_text]\n",
    "    print \"Done tokenizing Viet users' posts\"\n",
    "    cat = [\"A\", \"B\", \"C\", \"D\"] \n",
    "    y = []\n",
    "    for i in range(len(y_cat)):\n",
    "        c = y_cat[i]\n",
    "        if c == cat[0]:\n",
    "            y.append([1, 0, 0, 0])\n",
    "        elif c == cat[1]:\n",
    "            y.append([0, 1, 0, 0])\n",
    "        elif c == cat[2]:\n",
    "            y.append([0, 0, 1, 0])\n",
    "        else:\n",
    "            y.append([0, 0, 0, 1])\n",
    "    #print X[0] \n",
    "    save_data_and_labels(X, y)\n",
    "    # Generate labels\n",
    "    return [X, y]\n",
    "\n",
    "\n",
    "def pad_sentences(sentences, labels, padding_word=\"<PAD/>\", min_length=200, max_length=800):\n",
    "    \"\"\"\n",
    "    Pads all sentences to the same length. The length is defined by the longest sentence.\n",
    "    Returns padded sentences.\n",
    "    \"\"\"\n",
    "    filtered_sentences = dict()\n",
    "    idxes = []\n",
    "    idx = 0\n",
    "    for s in sentences:\n",
    "        if len(s) > min_length:\n",
    "            if len(s) > max_length:\n",
    "                #print(\"Index: %s\" % idx)\n",
    "                composite_list = [s[x:x+max_length] for x in range(0, len(s),max_length) if len(s[x:x+max_length]) > 200]\n",
    "                filtered_sentences[idx] = (len(composite_list), composite_list)\n",
    "            else:\n",
    "                filtered_sentences[idx] = (1, [s])\n",
    "            idxes.append(idx)\n",
    "        idx += 1\n",
    "    print idxes\n",
    "    #print(\"There are %s padded sentences!\" % len(filtered_sentences.keys()))\n",
    "    #sequence_length = max(len(x) for values in filtered_sentences.values())\n",
    "    new_labels = []\n",
    "    original_sentences = []\n",
    "    for i in idxes:\n",
    "        num = filtered_sentences[i][0]\n",
    "        original_sentences += filtered_sentences[i][1]\n",
    "        new_labels += num * [labels[i]]\n",
    "    \n",
    "    padded_sentences = []\n",
    "    sequence_length = max(len(x) for x in original_sentences)\n",
    "    for i in range(len(original_sentences)):\n",
    "        sentence = original_sentences[i]\n",
    "        num_padding = sequence_length - len(sentence)\n",
    "        new_sentence = sentence + [padding_word] * num_padding\n",
    "        padded_sentences.append(new_sentence)\n",
    "    \n",
    "    print(\"There are %s padded sentences\" % len(padded_sentences))\n",
    "    return [padded_sentences, new_labels]\n",
    "\n",
    "\n",
    "def build_vocab(sentences):\n",
    "    \"\"\"\n",
    "    Builds a vocabulary mapping from word to index based on the sentences.\n",
    "    Returns vocabulary mapping and inverse vocabulary mapping.\n",
    "    \"\"\"\n",
    "    # Build vocabulary\n",
    "    word_counts = Counter(itertools.chain(*sentences))\n",
    "    # Mapping from index to word\n",
    "    vocabulary_inv_list = [x[0] for x in word_counts.most_common()]\n",
    "    # Mapping from word to index\n",
    "    vocabulary = {x: i for i, x in enumerate(vocabulary_inv_list)}\n",
    "    vocabulary_inv = {key: value for key, value in enumerate(vocabulary_inv_list)}\n",
    "    with open('vocabularies.json', 'w') as fv:\n",
    "        json.dump(vocabulary, fv)\n",
    "    with open('vocabularies_inv.json', 'w') as fvi:\n",
    "        json.dump(vocabulary_inv, fvi)\n",
    "    return [vocabulary, vocabulary_inv]\n",
    "\n",
    "\n",
    "def build_input_data(sentences, labels, vocabulary):\n",
    "    \"\"\"\n",
    "    Maps sentencs and labels to vectors based on a vocabulary.\n",
    "    \"\"\"\n",
    "    x = np.array([[vocabulary[word] for word in sentence] for sentence in sentences])\n",
    "    y = np.array(labels)\n",
    "    print len(x)\n",
    "    print len(y)\n",
    "    np.save('/data_x_new_200_800.npy', x)\n",
    "    np.save('/data_y_new_200_800.npy', y)\n",
    "    return [x, y]\n",
    "\n",
    "\n",
    "def preprocess_data():\n",
    "    \"\"\"\n",
    "    Loads and preprocessed data for the MR dataset.\n",
    "    Returns input vectors, labels, vocabulary, and inverse vocabulary.\n",
    "    \"\"\"\n",
    "    # Load and preprocess data\n",
    "    sentences, labels = text_and_labels()\n",
    "    print len(sentences)\n",
    "    print \"finish loading tokenized sentences and labels\"\n",
    "    sentences_padded, filtered_labels = pad_sentences(sentences, labels)\n",
    "    #print len(sentences_padded)\n",
    "    #filtered_labels = labels[idxes]\n",
    "    print \"finish padding\"\n",
    "    vocabulary, vocabulary_inv = build_vocab(sentences_padded)\n",
    "    '''\n",
    "    with open('vocabularies.json', 'r') as fv:\n",
    "        vocabulary = json.load(fv)\n",
    "    with open('vocabularies_inv.json', 'r') as fvi:\n",
    "        vocabulary_inv = json.load(fvi)\n",
    "    '''\n",
    "    print \"finish building vocabularies\"\n",
    "    x, y = build_input_data(sentences_padded, filtered_labels, vocabulary)\n",
    "    print \"finish mapping sentences and labels to vectors based on vocabularies\"\n",
    "    return [x, y, vocabulary, vocabulary_inv]\n",
    "\n",
    "\n",
    "def batch_iter(data, batch_size, num_epochs):\n",
    "    \"\"\"\n",
    "    Generates a batch iterator for a dataset.\n",
    "    \"\"\"\n",
    "    data = np.array(data)\n",
    "    data_size = len(data)\n",
    "    num_batches_per_epoch = int(len(data) / batch_size) + 1\n",
    "    for epoch in range(num_epochs):\n",
    "        # Shuffle the data at each epoch\n",
    "        shuffle_indices = np.random.permutation(np.arange(data_size))\n",
    "        shuffled_data = data[shuffle_indices]\n",
    "        for batch_num in range(num_batches_per_epoch):\n",
    "            start_index = batch_num * batch_size\n",
    "            end_index = min((batch_num + 1) * batch_size, data_size)\n",
    "            yield shuffled_data[start_index:end_index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "load_data()\n",
    "with open('age_data.csv', 'wb') as csv_file:\n",
    "    writer = csv.writer(csv_file)\n",
    "    for r in results:\n",
    "        writer.writerow([r])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def checking_file(filename):\n",
    "    train_df = pd.read_pickle(filename)\n",
    "    #train_df = train_df.dropna()\n",
    "    #train_df['post'] = map(lambda x: x.decode('utf-8'), train_df['userposts'])\n",
    "    t_sentences = train_df.loc[:, 'userposts']\n",
    "    max_len = -1    \n",
    "    min_len = sys.maxint    \n",
    "    length = 0     \n",
    "    for ts in t_sentences:        \n",
    "        l = len(ts)\n",
    "        if l > max_len:\n",
    "            max_len = l\n",
    "        if l < min_len:\n",
    "            min_len = l \n",
    "        length += l        \n",
    "    avg_len = length/len(t_sentences)       \n",
    "    print (min_len, avg_len, max_len)\n",
    "    return t_sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#data_load()\n",
    "import sys\n",
    "df = pd.read_csv(SOURCE_DIR + 'post_user_from_04_2017.csv')\n",
    "print len(df.index)\n",
    "#load_data()\n",
    "checking_file()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "checking_file()\n",
    "'''\n",
    "with open(\"./output_sample.csv\",'wb') as resultFile:\n",
    "    wr = csv.writer(resultFile, dialect='excel', encoding='utf-8')\n",
    "    wr.writerows(results)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ViTokenizer.tokenize(u\"Trường đại học bách khoa hà nội\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "l = []\n",
    "string = u\"Trường đại học bách khoa hà nội 01650833798\"\n",
    "l.extend(ViTokenizer.tokenize(string).split(' '))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print l\n",
    "print tokenize(string)[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x_text = clean_str(u\"Trường đại học bách khoa hà nội 0910833798\")\n",
    "#x_text = re.sub(r\"[^A-Za-z0-9(),!?\\'\\`]\", \" \", x_text)\n",
    "#X = [s.split(\" \") for s in x_text]\n",
    "print X\n",
    "#print X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x_text = [u\"Trường đại học bách khoa hà nội 0910833798\", u\"Trường đại học bách khoa hà nội 0910833798\"]\n",
    "x_text = [s.split(\" \") for s in x_text]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print x_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocess_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X, y = text_and_labels()\n",
    "for i in range(3):\n",
    "    print (X[i], y[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print X[0][4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocess_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "load_data_and_labels()\n",
    "print \"finish loading tokenized sentences and labels\"\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "t_sentences = checking_file('./tokenized_original.pkl')\n",
    "t_sentences_s = checking_file('./tokenized_sample.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "small_files = [f for f in t_sentences_s if len(f) > 100]\n",
    "avg_files = [f for f in t_sentences_s if len(f) > 500]\n",
    "large_files = [f for f in t_sentences_s if len(f) > 150 and len(f) < 4000]\n",
    "print \"# of Files with small sizes: %s\" % len(small_files)\n",
    "print \"# of Files with average sizes: %s\" % len(avg_files)\n",
    "print \"# of Files with large sizes: %s\" % len(large_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "'''\n",
    "def pad_sentences(sentences, padding_word=\"<PAD/>\", min_length=100, max_length=4000):\n",
    "    \"\"\"\n",
    "    Pads all sentences to the same length. The length is defined by the longest sentence.\n",
    "    Returns padded sentences.\n",
    "    \"\"\"\n",
    "    filtered_sentences = []\n",
    "    idxes = []\n",
    "    idx = 0\n",
    "    for s in sentences:\n",
    "        if len(s) > min_length:\n",
    "            if len(s) > max_length:\n",
    "                filtered_sentences.append(s[:4000])\n",
    "            else:\n",
    "                filtered_sentences.append(s)\n",
    "            idxes.append(idx)\n",
    "        idx += 1\n",
    "    print(\"There are %s padded sentences!\" % len(filtered_sentences))\n",
    "    sequence_length = max(len(x) for x in filtered_sentences)\n",
    "    padded_sentences = []\n",
    "    for i in range(len(filtered_sentences)):\n",
    "        sentence = filtered_sentences[i]\n",
    "        num_padding = sequence_length - len(sentence)\n",
    "        new_sentence = sentence + [padding_word] * num_padding\n",
    "        padded_sentences.append(new_sentence)\n",
    "    return padded_sentences, idxes\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sentences, labels = text_and_labels()\n",
    "print len(sentences)\n",
    "print \"finish loading tokenized sentences and labels\"\n",
    "filtered_sentences, sentences_padded, filtered_labels = pad_sentences(sentences, labels)\n",
    "   #print len(sentences_padded)\n",
    "    #filtered_labels = labels[idxes]\n",
    "print \"finish padding\"\n",
    "vocabulary, vocabulary_inv = build_vocab(sentences_padded)\n",
    "'''\n",
    "    with open('vocabularies.json', 'r') as fv:\n",
    "        vocabulary = json.load(fv)\n",
    "    with open('vocabularies_inv.json', 'r') as fvi:\n",
    "        vocabulary_inv = json.load(fvi)\n",
    "'''\n",
    "print \"finish building vocabularies\"\n",
    "x, y = build_input_data(sentences_padded, filtered_labels, vocabulary)\n",
    "print \"finish mapping sentences and labels to vectors based on vocabularies\"\n",
    "#return [x, y, vocabulary, vocabulary_inv]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "idx = 0\n",
    "for i in sentences_padded:\n",
    "    if idx < 4:    \n",
    "        print i\n",
    "    else:\n",
    "        break\n",
    "    idx += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(sentences_padded[5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print filtered_sentences[5][1][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "a = itertools.chain(*sentences_padded)\n",
    "for i in a:\n",
    "    print i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Counter([1, 3, 4, 5, 3, 1])\n",
    "print(type(list(list())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print a[4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "for i in a:\n",
    "    if type(i) == type(list()):\n",
    "        print i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
