{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from keras.backend.tensorflow_backend import set_session\n",
    "config = tf.ConfigProto()\n",
    "config.gpu_options.per_process_gpu_memory_fraction = 0.75\n",
    "set_session(tf.Session(config=config))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import LSTM, Convolution1D, MaxPooling1D, AveragePooling1D, GlobalMaxPooling1D, Flatten, Dropout, Input, Merge\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.preprocessing import sequence\n",
    "from keras.callbacks import TensorBoard\n",
    "from keras.utils.np_utils import to_categorical\n",
    "from sklearn.utils import class_weight\n",
    "import pickle\n",
    "import json\n",
    "from __future__ import print_function\n",
    "from gensim.models import word2vec\n",
    "from os.path import join, exists, split\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_word2vec(sentence_matrix, vocabulary_inv,\n",
    "                   num_features=100, min_word_count=1, context=10):\n",
    "    \"\"\"\n",
    "    Trains, saves, loads Word2Vec model\n",
    "    Returns initial weights for embedding layer.\n",
    "   \n",
    "    inputs:\n",
    "    sentence_matrix # int matrix: num_sentences x max_sentence_len\n",
    "    vocabulary_inv  # dict {int: str}\n",
    "    num_features    # Word vector dimensionality                      \n",
    "    min_word_count  # Minimum word count                        \n",
    "    context         # Context window size \n",
    "    \"\"\"\n",
    "    model_dir = 'models'\n",
    "    model_name = \"{:d}features_{:d}minwords_{:d}context\".format(num_features, min_word_count, context)\n",
    "    model_name = join(model_dir, model_name)\n",
    "    if exists(model_name):\n",
    "        embedding_model = word2vec.Word2Vec.load(model_name)\n",
    "        print('Load existing Word2Vec model \\'%s\\'' % split(model_name)[-1])\n",
    "    else:\n",
    "        # Set values for various parameters\n",
    "        num_workers = 2  # Number of threads to run in parallel\n",
    "        downsampling = 1e-3  # Downsample setting for frequent words\n",
    "\n",
    "        # Initialize and train the model\n",
    "        print('Training Word2Vec model...')\n",
    "        sentences = [[vocabulary_inv[str(w)] for w in s] for s in sentence_matrix]\n",
    "        embedding_model = word2vec.Word2Vec(sentences, workers=num_workers,\n",
    "                                            size=num_features, min_count=min_word_count,\n",
    "                                            window=context, sample=downsampling)\n",
    "\n",
    "        # If we don't plan to train the model any further, calling \n",
    "        # init_sims will make the model much more memory-efficient.\n",
    "        embedding_model.init_sims(replace=True)\n",
    "\n",
    "        # Saving the model for later use. You can load it later using Word2Vec.load()\n",
    "        if not exists(model_dir):\n",
    "            os.mkdir(model_dir)\n",
    "        print('Saving Word2Vec model \\'%s\\'' % split(model_name)[-1])\n",
    "        embedding_model.save(model_name)\n",
    "    \n",
    "    for i, word in enumerate(embedding_model.wv.index2word):\n",
    "        if i < 3:\n",
    "            print(\"Index: %s Word: %s\" % (i, word))\n",
    "\n",
    "    print(\"First embedding word: %s\" % embedding_model.wv.index2word[1])\n",
    "    print(\"Embedding vector of the word (và): %s\", embedding_model.wv[u'và'])\n",
    "    \n",
    "    # add unknown words\n",
    "    embedding_weights = {key: embedding_model[word] if word in embedding_model else\n",
    "                              np.random.uniform(-0.25, 0.25, embedding_model.vector_size)\n",
    "                         for key, word in vocabulary_inv.items()}\n",
    "    return embedding_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22694\n",
      "22694\n",
      "Age range A has: 5465. 0.379289451942% of the dataset\n",
      "Age range B has: 7837. 0.536821812173% of the dataset\n",
      "Age range C has: 3957. 0.27243183696% of the dataset\n",
      "Age range D has: 896. 0.0614706692371% of the dataset\n",
      "[[0 1 0 0]\n",
      " [0 1 0 0]\n",
      " [1 0 0 0]\n",
      " ..., \n",
      " [1 0 0 0]\n",
      " [0 0 1 0]\n",
      " [1 0 0 0]]\n",
      "Finish splitting data\n",
      "\n",
      "Adjusting sequence length for actual size\n",
      "X_train shape: (14524, 800)\n",
      "X_validate shape: (3631, 800)\n",
      "X_test shape: (4539, 800)\n",
      "Vocabulary Size: 252081\n",
      "[ 1.  0.  0.  0.]\n",
      "[ 0.  0.  0.  1.]\n",
      "[ 0.  1.  0.  0.]\n",
      "Loading data...\n"
     ]
    }
   ],
   "source": [
    "# Using keras to load the dataset with the top_words\n",
    "with open(r\"data_x_new_200_800.pkl\", \"rb\") as load_x:\n",
    "    x = pickle.load(load_x)\n",
    "with open(r\"data_y_new_200_800.pkl\", \"rb\") as load_y:\n",
    "    y = pickle.load(load_y)\n",
    "    \n",
    "with open('vocabularies.json', 'r') as fv:\n",
    "    vocabulary = json.load(fv)\n",
    "with open('vocabularies_inv.json', 'r') as fvi:\n",
    "    vocabulary_inv = json.load(fvi)\n",
    "    \n",
    "# Shuffle data\n",
    "np.random.seed(47)\n",
    "print(len(x))\n",
    "print(len(y))\n",
    "\n",
    "shuffle_indices = np.random.permutation(np.arange(len(y)))\n",
    "\n",
    "x = x[shuffle_indices]\n",
    "y = y[shuffle_indices]\n",
    "x = np.asarray(x)\n",
    "Y = []\n",
    "for entry in y:\n",
    "    Y.append(np.argmax(entry))\n",
    "train_len = int(len(x) * 0.80)\n",
    "validate_len = int(train_len * 0.2)\n",
    "print(\"Age range A has: %s. %s%% of the dataset\" % (len([1 for entry in Y[:train_len] if entry == 0]), float(len([1 for entry in Y if entry == 0]))/len(Y[:train_len])))\n",
    "print(\"Age range B has: %s. %s%% of the dataset\" % (len([1 for entry in Y[:train_len] if entry == 1]), float(len([1 for entry in Y if entry == 1]))/len(Y[:train_len])))\n",
    "print(\"Age range C has: %s. %s%% of the dataset\" % (len([1 for entry in Y[:train_len] if entry == 2]), float(len([1 for entry in Y if entry == 2]))/len(Y[:train_len])))\n",
    "print(\"Age range D has: %s. %s%% of the dataset\" % (len([1 for entry in Y[:train_len] if entry == 3]), float(len([1 for entry in Y if entry == 3]))/len(Y[:train_len])))\n",
    "#class_weights = class_weight.compute_class_weight('balanced', np.unique(Y_train), list(Y_train))\n",
    "#print(get_class_weights(Y[:train_len]))\n",
    "#print(class_weights)\n",
    "print(y)\n",
    "Y = to_categorical(Y)\n",
    "X_train = x[:train_len][validate_len:]\n",
    "y_train = Y[:train_len][validate_len:]\n",
    "X_validate = x[:train_len][:validate_len]\n",
    "y_validate = Y[:train_len][:validate_len]\n",
    "X_test = x[train_len:]\n",
    "y_test = Y[train_len:]\n",
    "word2vec_f = False\n",
    "\n",
    "print(\"Finish splitting data\")\n",
    "print()\n",
    "\n",
    "sequence_length = 1000\n",
    "\n",
    "if sequence_length != X_test.shape[1]:\n",
    "    print(\"Adjusting sequence length for actual size\")\n",
    "    sequence_length = X_test.shape[1]\n",
    "\n",
    "print(\"X_train shape:\", X_train.shape)\n",
    "print(\"X_validate shape:\", X_validate.shape)\n",
    "print(\"X_test shape:\", X_test.shape)\n",
    "print(\"Vocabulary Size: {:d}\".format(len(vocabulary_inv)))\n",
    "\n",
    "for i in range(3):\n",
    "    print(y_train[i])\n",
    "    \n",
    "print(\"Loading data...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "balance_klass = SMOTE()\n",
    "l = {0: 2, 1: 3}\n",
    "X_train_smote, y_train_smote = balance_klass.execute(l, samples=X_train, labels=y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "if word2vec_f:\n",
    "    w = train_word2vec(np.vstack((x[:train_len], X_test)), vocabulary_inv, num_features=200, min_word_count=1, context=15)\n",
    "#model_name = \"vi.bin\"\n",
    "#model_temp = word2vec.Word2Vec.load(\"./models/\" + model_name)\n",
    "#print('Load existing Word2Vec model \\'%s\\'' % split(model_name)[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "if word2vec_f:\n",
    "    x_train_new = np.stack([np.stack([w[str(word)] for word in sentence]) for sentence in X_train_smote])\n",
    "    x_validate_new = np.stack([np.stack([w[str(word)] for word in sentence]) for sentence in X_validate])\n",
    "    x_test_new = np.stack([np.stack([w[str(word)] for word in sentence]) for sentence in X_test])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "if word2vec_f:\n",
    "    #save and load data in order to save space \n",
    "    #in case you need them specifically for when computing word2vec  \n",
    "    print(x_train_new.shape)\n",
    "    np.save('/train_x_1.npy', x_train_new)\n",
    "    np.save('/validate_x_1.npy', x_validate_new)\n",
    "    np.save('/test_x_1.npy', x_test_new)\n",
    "    x_train_new = np.load('/train_x_1.npy')\n",
    "    x_validate_new = np.load('/validate_x_1.npy')\n",
    "    x_test_new = np.load('/test_x_1.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(14524, 800)\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_5 (Embedding)      (None, 800, 8)            2016648   \n",
      "_________________________________________________________________\n",
      "conv1d_10 (Conv1D)           (None, 800, 96)           2400      \n",
      "_________________________________________________________________\n",
      "dropout_13 (Dropout)         (None, 800, 96)           0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_9 (MaxPooling1 (None, 400, 96)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_11 (Conv1D)           (None, 400, 64)           12352     \n",
      "_________________________________________________________________\n",
      "dropout_14 (Dropout)         (None, 400, 64)           0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_10 (MaxPooling (None, 200, 64)           0         \n",
      "_________________________________________________________________\n",
      "flatten_5 (Flatten)          (None, 12800)             0         \n",
      "_________________________________________________________________\n",
      "dense_9 (Dense)              (None, 32)                409632    \n",
      "_________________________________________________________________\n",
      "dropout_15 (Dropout)         (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "dense_10 (Dense)             (None, 4)                 132       \n",
      "=================================================================\n",
      "Total params: 2,441,164\n",
      "Trainable params: 2,441,164\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/huyqt7/miniconda2/lib/python2.7/site-packages/ipykernel_launcher.py:23: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(4, activation=\"softmax\", kernel_regularizer=<keras.reg...)`\n"
     ]
    }
   ],
   "source": [
    "if not word2vec_f:\n",
    "    # Using embedding from Keras\n",
    "    from keras.regularizers import l2\n",
    "    model = Sequential()\n",
    "    print(X_train.shape)\n",
    "    model.add(Embedding(len(vocabulary_inv), 8, input_length=sequence_length))\n",
    "\n",
    "    model.add(Convolution1D(96, activation='relu', kernel_size=3, padding='same'))\n",
    "    model.add(Dropout(0.4))\n",
    "    model.add(MaxPooling1D(pool_size=2))\n",
    "    model.add(Convolution1D(64, activation='relu', kernel_size=2, strides = 1, padding='same'))\n",
    "    model.add(Dropout(0.4))\n",
    "    model.add(MaxPooling1D(pool_size=2))\n",
    "\n",
    "    #model.add(Convolution1D(192, activation='relu', kernel_size=3, padding='same'))\n",
    "    #model.add(Convolution1D(192, activation='relu', kernel_size=1, padding='same'))\n",
    "    #model.add(Convolution1D(64, kernel_size=1))\n",
    "    #model.add(AveragePooling1D(pool_size=8, strides=1))\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(32, activation='relu'))\n",
    "    model.add(Dropout(0.25))\n",
    "    #model.add(LSTM(128, dropout=0.3, recurrent_dropout=0.3))\n",
    "    model.add(Dense(4, activation='softmax', W_regularizer=l2(0.01)))\n",
    "\n",
    "    # Log to tensorboard\n",
    "    #tensorBoardCallback = TensorBoard(log_dir='./logs', write_graph=True)\n",
    "    #model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "    print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "if word2vec_f:\n",
    "    # Using embedding from Keras\n",
    "    from keras.regularizers import l2\n",
    "\n",
    "    embedding_dim = 100\n",
    "    embedding_vecor_length = 300\n",
    "    model = Sequential()\n",
    "\n",
    "    #Convolutional model (3x conv, flatten, 2x dense)\n",
    "    #input_shape=(sequence_length, 100), \n",
    "    model.add(Convolution1D(512, input_shape=(sequence_length, 200), kernel_size=3, activation=\"relu\", padding='same'))\n",
    "    #model.add(Convolution1D(128, kernel_size=3, activation=\"relu\", padding='same'))\n",
    "    model.add(MaxPooling1D(pool_size=3))\n",
    "    model.add(Dropout(0.25))\n",
    "    model.add(Convolution1D(256, input_shape=(sequence_length, 200), kernel_size=3, activation=\"relu\", padding='same'))\n",
    "    model.add(MaxPooling1D(pool_size=2))\n",
    "    model.add(Convolution1D(128, kernel_size=3, strides = 1, padding='same'))\n",
    "    model.add(Dropout(0.25))\n",
    "    model.add(MaxPooling1D(pool_size=1))\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(128, activation='relu'))\n",
    "    model.add(Dropout(0.25))\n",
    "    model.add(Dense(4, activation='softmax'))\n",
    "    #model.add(Dense(4, activation='softmax', W_regularizer=l2(0.01)))\n",
    "\n",
    "    # Log to tensorboard\n",
    "    #tensorBoardCallback = TensorBoard(log_dir='./logs', write_graph=True)\n",
    "    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "    print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running without word2vec\n",
      "Train on 14524 samples, validate on 3631 samples\n",
      "Epoch 1/10\n",
      " - 8s - loss: 1.1732 - acc: 0.4351 - val_loss: 1.0303 - val_acc: 0.5059\n",
      "Epoch 2/10\n",
      " - 8s - loss: 0.8864 - acc: 0.5914 - val_loss: 0.8168 - val_acc: 0.6615\n",
      "Epoch 3/10\n",
      " - 8s - loss: 0.5785 - acc: 0.7572 - val_loss: 0.7368 - val_acc: 0.6910\n",
      "Epoch 4/10\n",
      " - 8s - loss: 0.3413 - acc: 0.8746 - val_loss: 0.8169 - val_acc: 0.7050\n",
      "Epoch 5/10\n",
      " - 8s - loss: 0.2141 - acc: 0.9305 - val_loss: 1.0291 - val_acc: 0.6962\n",
      "Epoch 6/10\n",
      " - 8s - loss: 0.1456 - acc: 0.9566 - val_loss: 1.0884 - val_acc: 0.7009\n",
      "Epoch 7/10\n",
      " - 8s - loss: 0.1119 - acc: 0.9699 - val_loss: 1.3353 - val_acc: 0.7001\n",
      "Epoch 8/10\n"
     ]
    }
   ],
   "source": [
    "#y_train = y_train.reshape((-1, 1))\n",
    "if word2vec_f:\n",
    "    print(\"Combining with word2vec model!\")\n",
    "    model.fit(x_train_new, y_train, epochs=10, validation_data=(x_validate_new, y_validate), batch_size=64, verbose=2) #class_weight={0:0.82856529, 1:0.57931884, 2:1.14952324, 3:5.07605263})\n",
    "    #{0:0.82704321, 1:0.58675982, 2:1.13997847, 3:4.7758016})\n",
    "else:\n",
    "    print(\"Running without word2vec\")\n",
    "    model.fit(X_train, y_train, epochs=10, validation_data=(X_validate, y_validate), batch_size=64, verbose=2) #class_weight={0:0.82856529, 1:0.57931884, 2:1.14952324, 3:5.07605263})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4539/4539 [==============================] - 0s 99us/step\n",
      "Accuracy: 71.03%\n"
     ]
    }
   ],
   "source": [
    "# Evaluation on the test set\n",
    "if word2vec_f:\n",
    "    scores = model.evaluate(x_test_new, y_test)\n",
    "else:\n",
    "    scores = model.evaluate(X_test, y_test)\n",
    "print(\"Accuracy: %.2f%%\" % (scores[1]*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#in case you want to run LSTM model on this project\n",
    "#too much time intensive \n",
    "model.add(Embedding(len(vocabulary_inv), 100, input_length = sequence_length))\n",
    "model.add(Dropout(0.4))\n",
    "model.add(LSTM(128, dropout=0.2, recurrent_dropout=0.2))\n",
    "model.add(Dense(4, activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "for i, word in enumerate(model_temp.index2word):\n",
    "    if i < 3:\n",
    "        print(\"Index: %s Word: %s\" % (i, word))\n",
    "\n",
    "print(model_temp.wv.index2word[1])\n",
    "print(model_temp.wv[u'và'])\n",
    "index = 0\n",
    "\n",
    "embedding_weights = {key: model_temp.wv[word] if word in model_temp.index2word else\n",
    "                        np.random.uniform(-0.25, 0.25, model_temp.vector_size)\n",
    "                        for key, word in vocabulary_inv.items()}\n",
    "                        \n",
    "nb_words = min(vocabulary_inv)+1\n",
    "\n",
    "embedding_matrix = np.zeros((nb_words, EMBEDDING_DIM))\n",
    "for word, i in word_index.items():\n",
    "    if word in word2vec.vocab:\n",
    "        embedding_matrix[i] = word2vec.word_vec(word)\n",
    "print('Null word embeddings: %d' % np.sum(np.sum(embedding_matrix, axis=1) == 0))\n",
    "'''"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
