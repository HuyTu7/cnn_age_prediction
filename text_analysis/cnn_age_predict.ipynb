{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Train convolutional network for sentiment analysis on IMDB corpus. Based on\n",
    "\"Convolutional Neural Networks for Sentence Classification\" by Yoon Kim\n",
    "http://arxiv.org/pdf/1408.5882v2.pdf\n",
    "For \"CNN-rand\" and \"CNN-non-static\" gets to 88-90%, and \"CNN-static\" - 85% after 2-5 epochs with following settings:\n",
    "embedding_dim = 50          \n",
    "filter_sizes = (3, 8)\n",
    "num_filters = 10\n",
    "dropout_prob = (0.5, 0.8)\n",
    "hidden_dims = 50\n",
    "Differences from original article:\n",
    "- larger IMDB corpus, longer sentences; sentence length is very important, just like data size\n",
    "- smaller embedding dimension, 50 instead of 300\n",
    "- 2 filter sizes instead of original 3\n",
    "- fewer filters; original work uses 100, experiments show that 3-10 is enough;\n",
    "- random initialization is no worse than word2vec init on IMDB corpus\n",
    "- sliding Max Pooling instead of original Global Pooling\n",
    "\"\"\"\n",
    "\n",
    "import numpy as np\n",
    "import data_helpers\n",
    "from w2v import train_word2vec\n",
    "\n",
    "from keras.models import Sequential, Model\n",
    "from keras.layers import Dense, Dropout, Flatten, Input, MaxPooling1D, Convolution1D, Embedding\n",
    "from keras.layers.merge import Concatenate\n",
    "from keras.datasets import imdb\n",
    "from keras.preprocessing import sequence\n",
    "np.random.seed(0)\n",
    "\n",
    "# ---------------------- Parameters section -------------------\n",
    "#\n",
    "# Model type. See Kim Yoon's Convolutional Neural Networks for Sentence Classification, Section 3\n",
    "model_type = \"CNN-non-static\"  # CNN-rand|CNN-non-static|CNN-static\n",
    "\n",
    "# Data source\n",
    "data_source = \"keras_data_set\"  # keras_data_set|local_dir\n",
    "\n",
    "# Model Hyperparameters\n",
    "embedding_dim = 50\n",
    "filter_sizes = (3, 8)\n",
    "num_filters = 10\n",
    "dropout_prob = (0.5, 0.8)\n",
    "hidden_dims = 50\n",
    "\n",
    "# Training parameters\n",
    "batch_size = 64\n",
    "num_epochs = 10\n",
    "\n",
    "# Prepossessing parameters\n",
    "sequence_length = 400\n",
    "max_words = 5000\n",
    "\n",
    "# Word2Vec parameters (see train_word2vec)\n",
    "min_word_count = 1\n",
    "context = 10\n",
    "\n",
    "#\n",
    "# ---------------------- Parameters end -----------------------\n",
    "\n",
    "\n",
    "def load_data(data_source):\n",
    "    assert data_source in [\"keras_data_set\", \"local_dir\"], \"Unknown data source\"\n",
    "    if data_source == \"keras_data_set\":\n",
    "        (x_train, y_train), (x_test, y_test) = imdb.load_data(num_words=max_words, start_char=None,\n",
    "                                                              oov_char=None, index_from=None)\n",
    "\n",
    "        x_train = sequence.pad_sequences(x_train, maxlen=sequence_length, padding=\"post\", truncating=\"post\")\n",
    "        x_test = sequence.pad_sequences(x_test, maxlen=sequence_length, padding=\"post\", truncating=\"post\")\n",
    "\n",
    "        vocabulary = imdb.get_word_index()\n",
    "        vocabulary_inv = dict((v, k) for k, v in vocabulary.items())\n",
    "        vocabulary_inv[0] = \"<PAD/>\"\n",
    "    else:\n",
    "        x, y, vocabulary, vocabulary_inv_list = data_helpers.load_data()\n",
    "        vocabulary_inv = {key: value for key, value in enumerate(vocabulary_inv_list)}\n",
    "        y = y.argmax(axis=1)\n",
    "\n",
    "        # Shuffle data\n",
    "        shuffle_indices = np.random.permutation(np.arange(len(y)))\n",
    "        x = x[shuffle_indices]\n",
    "        y = y[shuffle_indices]\n",
    "        train_len = int(len(x) * 0.9)\n",
    "        x_train = x[:train_len]\n",
    "        y_train = y[:train_len]\n",
    "        x_test = x[train_len:]\n",
    "        y_test = y[train_len:]\n",
    "\n",
    "    return x_train, y_train, x_test, y_test, vocabulary_inv\n",
    "\n",
    "\n",
    "# Data Preparation\n",
    "print(\"Load data...\")\n",
    "x_train, y_train, x_test, y_test, vocabulary_inv = load_data(data_source)\n",
    "\n",
    "if sequence_length != x_test.shape[1]:\n",
    "    print(\"Adjusting sequence length for actual size\")\n",
    "    sequence_length = x_test.shape[1]\n",
    "\n",
    "print(\"x_train shape:\", x_train.shape)\n",
    "print(\"x_test shape:\", x_test.shape)\n",
    "print(\"Vocabulary Size: {:d}\".format(len(vocabulary_inv)))\n",
    "\n",
    "# Prepare embedding layer weights and convert inputs for static model\n",
    "print(\"Model type is\", model_type)\n",
    "if model_type in [\"CNN-non-static\", \"CNN-static\"]:\n",
    "    embedding_weights = train_word2vec(np.vstack((x_train, x_test)), vocabulary_inv, num_features=embedding_dim,\n",
    "                                       min_word_count=min_word_count, context=context)\n",
    "    if model_type == \"CNN-static\":\n",
    "        x_train = np.stack([np.stack([embedding_weights[word] for word in sentence]) for sentence in x_train])\n",
    "        x_test = np.stack([np.stack([embedding_weights[word] for word in sentence]) for sentence in x_test])\n",
    "        print(\"x_train static shape:\", x_train.shape)\n",
    "        print(\"x_test static shape:\", x_test.shape)\n",
    "\n",
    "elif model_type == \"CNN-rand\":\n",
    "    embedding_weights = None\n",
    "else:\n",
    "    raise ValueError(\"Unknown model type\")\n",
    "\n",
    "# Build model\n",
    "if model_type == \"CNN-static\":\n",
    "    input_shape = (sequence_length, embedding_dim)\n",
    "else:\n",
    "    input_shape = (sequence_length,)\n",
    "\n",
    "model_input = Input(shape=input_shape)\n",
    "\n",
    "# Static model does not have embedding layer\n",
    "if model_type == \"CNN-static\":\n",
    "    z = model_input\n",
    "else:\n",
    "    z = Embedding(len(vocabulary_inv), embedding_dim, input_length=sequence_length, name=\"embedding\")(model_input)\n",
    "\n",
    "z = Dropout(dropout_prob[0])(z)\n",
    "\n",
    "# Convolutional block\n",
    "conv_blocks = []\n",
    "for sz in filter_sizes:\n",
    "    conv = Convolution1D(filters=num_filters,\n",
    "                         kernel_size=sz,\n",
    "                         padding=\"valid\",\n",
    "                         activation=\"relu\",\n",
    "                         strides=1)(z)\n",
    "    conv = MaxPooling1D(pool_size=2)(conv)\n",
    "    conv = Flatten()(conv)\n",
    "    conv_blocks.append(conv)\n",
    "z = Concatenate()(conv_blocks) if len(conv_blocks) > 1 else conv_blocks[0]\n",
    "\n",
    "z = Dropout(dropout_prob[1])(z)\n",
    "z = Dense(hidden_dims, activation=\"relu\")(z)\n",
    "model_output = Dense(1, activation=\"sigmoid\")(z)\n",
    "\n",
    "model = Model(model_input, model_output)\n",
    "model.compile(loss=\"binary_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"])\n",
    "\n",
    "# Initialize weights with word2vec\n",
    "if model_type == \"CNN-non-static\":\n",
    "    weights = np.array([v for v in embedding_weights.values()])\n",
    "    print(\"Initializing embedding layer with word2vec weights, shape\", weights.shape)\n",
    "    embedding_layer = model.get_layer(\"embedding\")\n",
    "    embedding_layer.set_weights([weights])\n",
    "\n",
    "# Train the model\n",
    "model.fit(x_train, y_train, batch_size=batch_size, epochs=num_epochs,\n",
    "          validation_data=(x_test, y_test), verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string \n",
    "text = u\"Trường đại học bách khoa hà nội 0910833798\"\n",
    "text.translate(None, string.punctuation)\n",
    "print text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_word2vec(sentence_matrix, vocabulary_inv,\n",
    "                   num_features=100, min_word_count=1, context=10):\n",
    "    \"\"\"\n",
    "    Trains, saves, loads Word2Vec model\n",
    "    Returns initial weights for embedding layer.\n",
    "   \n",
    "    inputs:\n",
    "    sentence_matrix # int matrix: num_sentences x max_sentence_len\n",
    "    vocabulary_inv  # dict {int: str}\n",
    "    num_features    # Word vector dimensionality                      \n",
    "    min_word_count  # Minimum word count                        \n",
    "    context         # Context window size \n",
    "    \"\"\"\n",
    "    model_dir = 'models'\n",
    "    model_name = \"{:d}features_{:d}minwords_{:d}context\".format(num_features, min_word_count, context)\n",
    "    model_name = join(model_dir, model_name)\n",
    "    if exists(model_name):\n",
    "        embedding_model = word2vec.Word2Vec.load(model_name)\n",
    "        print('Load existing Word2Vec model \\'%s\\'' % split(model_name)[-1])\n",
    "    else:\n",
    "        # Set values for various parameters\n",
    "        num_workers = 2  # Number of threads to run in parallel\n",
    "        downsampling = 1e-3  # Downsample setting for frequent words\n",
    "\n",
    "        # Initialize and train the model\n",
    "        print('Training Word2Vec model...')\n",
    "        sentences = [[vocabulary_inv[str(w)] for w in s] for s in sentence_matrix]\n",
    "        embedding_model = word2vec.Word2Vec(sentences, workers=num_workers,\n",
    "                                            size=num_features, min_count=min_word_count,\n",
    "                                            window=context, sample=downsampling)\n",
    "\n",
    "        # If we don't plan to train the model any further, calling \n",
    "        # init_sims will make the model much more memory-efficient.\n",
    "        embedding_model.init_sims(replace=True)\n",
    "\n",
    "        # Saving the model for later use. You can load it later using Word2Vec.load()\n",
    "        if not exists(model_dir):\n",
    "            os.mkdir(model_dir)\n",
    "        print('Saving Word2Vec model \\'%s\\'' % split(model_name)[-1])\n",
    "        embedding_model.save(model_name)\n",
    "\n",
    "    # add unknown words\n",
    "    embedding_weights = {key: embedding_model[word] if word in embedding_model else\n",
    "                              np.random.uniform(-0.25, 0.25, embedding_model.vector_size)\n",
    "                         for key, word in vocabulary_inv.items()}\n",
    "    return embedding_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3933\n",
      "3933\n",
      "1030\n",
      "1414\n",
      "1079\n",
      "410\n",
      "Finish splitting data\n",
      "\n",
      "Adjusting sequence length for actual size\n",
      "x_train shape: (3146, 3997)\n",
      "x_test shape: (787, 3997)\n",
      "Vocabulary Size: 125697\n",
      "[ 1.  0.  0.  0.]\n",
      "[ 0.  1.  0.  0.]\n",
      "[ 1.  0.  0.  0.]\n",
      "Loading data...\n",
      "Training Word2Vec model...\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "123",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-11-8759a6289f46>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     65\u001b[0m \u001b[0;32mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Loading data...\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 67\u001b[0;31m \u001b[0mw\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_word2vec\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvocabulary_inv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_features\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m50\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmin_word_count\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcontext\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-7-96edb03302fd>\u001b[0m in \u001b[0;36mtrain_word2vec\u001b[0;34m(sentence_matrix, vocabulary_inv, num_features, min_word_count, context)\u001b[0m\n\u001b[1;32m     25\u001b[0m         \u001b[0;31m# Initialize and train the model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m         \u001b[0;32mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Training Word2Vec model...'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m         \u001b[0msentences\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mvocabulary_inv\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mw\u001b[0m \u001b[0;32min\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0ms\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msentence_matrix\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     28\u001b[0m         embedding_model = word2vec.Word2Vec(sentences, workers=num_workers,\n\u001b[1;32m     29\u001b[0m                                             \u001b[0msize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnum_features\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmin_count\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmin_word_count\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 123"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import LSTM, Convolution1D, MaxPooling1D, Flatten, Dropout\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.preprocessing import sequence\n",
    "from keras.callbacks import TensorBoard\n",
    "from keras.utils.np_utils import to_categorical\n",
    "import json\n",
    "from __future__ import print_function\n",
    "from gensim.models import word2vec\n",
    "from os.path import join, exists, split\n",
    "import os\n",
    "\n",
    "# Using keras to load the dataset with the top_words\n",
    "x = np.load('/data_x.npy')\n",
    "y = np.load('/data_y.npy')\n",
    "with open('vocabularies.json', 'r') as fv:\n",
    "    vocabulary = json.load(fv)\n",
    "with open('vocabularies_inv.json', 'r') as fvi:\n",
    "    vocabulary_inv = json.load(fvi)\n",
    "    \n",
    "# Shuffle data\n",
    "np.random.seed(47)\n",
    "print(len(x))\n",
    "print(len(y))\n",
    "\n",
    "shuffle_indices = np.random.permutation(np.arange(len(y)))\n",
    "\n",
    "x = x[shuffle_indices]\n",
    "y = y[shuffle_indices]\n",
    "x = np.asarray(x)\n",
    "Y = []\n",
    "for entry in y:\n",
    "    Y.append(np.argmax(entry))\n",
    "print(len([1 for entry in Y if entry == 0]))\n",
    "print(len([1 for entry in Y if entry == 1]))\n",
    "print(len([1 for entry in Y if entry == 2]))\n",
    "print(len([1 for entry in Y if entry == 3]))\n",
    "\n",
    "Y = to_categorical(Y)\n",
    "\n",
    "train_len = int(len(x) * 0.8)\n",
    "X_train = x[:train_len]\n",
    "y_train = Y[:train_len]\n",
    "X_test = x[train_len:]\n",
    "y_test = Y[train_len:]\n",
    "\n",
    "print(\"Finish splitting data\")\n",
    "print()\n",
    "\n",
    "sequence_length = 4000\n",
    "\n",
    "if sequence_length != X_test.shape[1]:\n",
    "    print(\"Adjusting sequence length for actual size\")\n",
    "    sequence_length = X_test.shape[1]\n",
    "\n",
    "print(\"x_train shape:\", X_train.shape)\n",
    "print(\"x_test shape:\", X_test.shape)\n",
    "print(\"Vocabulary Size: {:d}\".format(len(vocabulary_inv)))\n",
    "\n",
    "for i in range(3):\n",
    "    print(Y[i])\n",
    "    \n",
    "print(\"Loading data...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Word2Vec model...\n",
      "Saving Word2Vec model '75features_3minwords_15context'\n"
     ]
    }
   ],
   "source": [
    "w = train_word2vec(np.vstack((X_train, X_test)), vocabulary_inv, num_features=75, min_word_count=3, context=15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_new = np.stack([np.stack([w[str(word)] for word in sentence]) for sentence in X_train])\n",
    "x_test_new = np.stack([np.stack([w[str(word)] for word in sentence]) for sentence in X_test])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_38 (Conv1D)           (None, 3997, 64)          14464     \n",
      "_________________________________________________________________\n",
      "dropout_33 (Dropout)         (None, 3997, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_39 (Conv1D)           (None, 3997, 32)          6176      \n",
      "_________________________________________________________________\n",
      "max_pooling1d_29 (MaxPooling (None, 1998, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_40 (Conv1D)           (None, 1998, 16)          1552      \n",
      "_________________________________________________________________\n",
      "max_pooling1d_30 (MaxPooling (None, 999, 16)           0         \n",
      "_________________________________________________________________\n",
      "flatten_13 (Flatten)         (None, 15984)             0         \n",
      "_________________________________________________________________\n",
      "dropout_34 (Dropout)         (None, 15984)             0         \n",
      "_________________________________________________________________\n",
      "dense_25 (Dense)             (None, 75)                1198875   \n",
      "_________________________________________________________________\n",
      "dropout_35 (Dropout)         (None, 75)                0         \n",
      "_________________________________________________________________\n",
      "dense_26 (Dense)             (None, 4)                 304       \n",
      "=================================================================\n",
      "Total params: 1,221,371\n",
      "Trainable params: 1,221,371\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/anaconda2/envs/tensorflow-cudnn/lib/python2.7/site-packages/ipykernel_launcher.py:9: UserWarning: Update your `Conv1D` call to the Keras 2 API: `Conv1D(64, activation=\"relu\", input_shape=(3997, 75), padding=\"same\", strides=1, kernel_size=3)`\n",
      "  if __name__ == '__main__':\n",
      "/root/anaconda2/envs/tensorflow-cudnn/lib/python2.7/site-packages/ipykernel_launcher.py:11: UserWarning: Update your `Conv1D` call to the Keras 2 API: `Conv1D(32, padding=\"same\", strides=1, activation=\"relu\", kernel_size=3)`\n",
      "  # This is added back by InteractiveShellApp.init_path()\n",
      "/root/anaconda2/envs/tensorflow-cudnn/lib/python2.7/site-packages/ipykernel_launcher.py:13: UserWarning: Update your `Conv1D` call to the Keras 2 API: `Conv1D(16, padding=\"same\", strides=1, activation=\"relu\", kernel_size=3)`\n",
      "  del sys.path[0]\n"
     ]
    }
   ],
   "source": [
    "# Using embedding from Keras\n",
    "embedding_dim = 150\n",
    "embedding_vecor_length = 300\n",
    "model = Sequential()\n",
    "#model.add(Embedding(len(vocabulary_inv), embedding_vecor_length, input_length=sequence_length))\n",
    "#model.add(LSTM(128, weights = w))\n",
    "\n",
    "# Convolutional model (3x conv, flatten, 2x dense)\n",
    "model.add(Convolution1D(64, input_shape=(sequence_length, 75), kernel_size=3, strides=1, activation=\"relu\", border_mode='same'))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Convolution1D(32, kernel_size=3, strides=1, activation=\"relu\", border_mode='same'))\n",
    "model.add(MaxPooling1D(pool_size=2))\n",
    "model.add(Convolution1D(16, kernel_size=3, strides=1, activation=\"relu\", border_mode='same'))\n",
    "model.add(MaxPooling1D(pool_size=2))\n",
    "model.add(Flatten())\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(75, activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(4, activation='softmax'))\n",
    "\n",
    "# Log to tensorboard\n",
    "tensorBoardCallback = TensorBoard(log_dir='./logs', write_graph=True)\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 3146 samples, validate on 787 samples\n",
      "Epoch 1/20\n",
      "3146/3146 [==============================] - 5s - loss: 1.3639 - acc: 0.3468 - val_loss: 1.3107 - val_acc: 0.3304\n",
      "Epoch 2/20\n",
      "3146/3146 [==============================] - 4s - loss: 1.3013 - acc: 0.3576 - val_loss: 1.2670 - val_acc: 0.3482\n",
      "Epoch 3/20\n",
      "3146/3146 [==============================] - 4s - loss: 1.2367 - acc: 0.3849 - val_loss: 1.2360 - val_acc: 0.3977\n",
      "Epoch 4/20\n",
      "3146/3146 [==============================] - 4s - loss: 1.2040 - acc: 0.4298 - val_loss: 1.2020 - val_acc: 0.4244\n",
      "Epoch 5/20\n",
      "3146/3146 [==============================] - 4s - loss: 1.1662 - acc: 0.4498 - val_loss: 1.2008 - val_acc: 0.4307\n",
      "Epoch 6/20\n",
      "3146/3146 [==============================] - 4s - loss: 1.1302 - acc: 0.4682 - val_loss: 1.1764 - val_acc: 0.4346\n",
      "Epoch 7/20\n",
      "3146/3146 [==============================] - 4s - loss: 1.1173 - acc: 0.4771 - val_loss: 1.1804 - val_acc: 0.4473\n",
      "Epoch 8/20\n",
      "3146/3146 [==============================] - 4s - loss: 1.0778 - acc: 0.4955 - val_loss: 1.1958 - val_acc: 0.4307\n",
      "Epoch 9/20\n",
      "3146/3146 [==============================] - 4s - loss: 1.0353 - acc: 0.5270 - val_loss: 1.2081 - val_acc: 0.4130\n",
      "Epoch 10/20\n",
      "3146/3146 [==============================] - 4s - loss: 0.9848 - acc: 0.5496 - val_loss: 1.2287 - val_acc: 0.4168\n",
      "Epoch 11/20\n",
      "3146/3146 [==============================] - 4s - loss: 0.9339 - acc: 0.5722 - val_loss: 1.2338 - val_acc: 0.4460\n",
      "Epoch 12/20\n",
      "3146/3146 [==============================] - 4s - loss: 0.9051 - acc: 0.5944 - val_loss: 1.3136 - val_acc: 0.4396\n",
      "Epoch 13/20\n",
      "3146/3146 [==============================] - 4s - loss: 0.8747 - acc: 0.6084 - val_loss: 1.3016 - val_acc: 0.4346\n",
      "Epoch 14/20\n",
      "3146/3146 [==============================] - 4s - loss: 0.8059 - acc: 0.6341 - val_loss: 1.4228 - val_acc: 0.4295\n",
      "Epoch 15/20\n",
      "3146/3146 [==============================] - 4s - loss: 0.7572 - acc: 0.6678 - val_loss: 1.4479 - val_acc: 0.4460\n",
      "Epoch 16/20\n",
      "3146/3146 [==============================] - 4s - loss: 0.7305 - acc: 0.6844 - val_loss: 1.4538 - val_acc: 0.4193\n",
      "Epoch 17/20\n",
      "3146/3146 [==============================] - 4s - loss: 0.6868 - acc: 0.7127 - val_loss: 1.5752 - val_acc: 0.4193\n",
      "Epoch 18/20\n",
      "3146/3146 [==============================] - 4s - loss: 0.6610 - acc: 0.7171 - val_loss: 1.5632 - val_acc: 0.4460\n",
      "Epoch 19/20\n",
      "3146/3146 [==============================] - 4s - loss: 0.6296 - acc: 0.7177 - val_loss: 1.5252 - val_acc: 0.4219\n",
      "Epoch 20/20\n",
      "1536/3146 [=============>................] - ETA: 2s - loss: 0.5856 - acc: 0.7604"
     ]
    }
   ],
   "source": [
    "model.fit(x_train_new, y_train, epochs=20, validation_data=(x_test_new, y_test), callbacks=[tensorBoardCallback], batch_size=64)\n",
    "\n",
    "# Evaluation on the test set\n",
    "scores = model.evaluate(x_test_new, y_test, verbose=0)\n",
    "print(\"Accuracy: %.2f%%\" % (scores[1]*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print np.load('/data_x.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
